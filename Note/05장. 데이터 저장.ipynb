{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05장. 데이터 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3가지 데이터 관리 방법\n",
    "\n",
    "1. 데이터 베이스 : 스크랩한 데이터를 웹사이트에 사용하거나 직접 API를 만드는 경우\n",
    "2. 파일 스트림 : 하드디스크에 저장할 쉽고 빠른 방법을 찾는 경우\n",
    "3. 이메일 전송 : 주기적 알림을 받거나 하루 한 번 데이터를 집계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미디어 파일\n",
    "\n",
    "1. 참조를 저장\n",
    "2. 파일 자체를 내려받기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('logo.jpg', <http.client.HTTPMessage at 0x23408062dd8>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com\")\n",
    "bs0bj = BeautifulSoup(html, \"html.parser\")\n",
    "imageLocation = bs0bj.find(\"a\", {\"id\": \"logo\"}).find(\"img\")[\"src\"]\n",
    "urlretrieve(imageLocation, \"logo.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 하나의 파일만 내려받았음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downloadDirectory = \"downloaded\"\n",
    "baseUrl = \"http://pythonscraping.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAbsoluteURL(baseUrl, source):\n",
    "    if source.startswith(\"http://www.\"):\n",
    "        url = \"http://\" + source[11:]\n",
    "    elif source.startswith(\"http://\"):\n",
    "        url = source\n",
    "    elif source.startswith(\"www.\"):\n",
    "        url = source[4:]\n",
    "        url = \"http://\" + source\n",
    "    else:\n",
    "        url = baseUrl + \"/\" + source\n",
    "    if baseUrl not in url:\n",
    "        return None\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDownloadPath(baseUrl, absoluteUrl, downloadDirectory):\n",
    "    path = absoluteUrl.replace(\"www.\", \"\")\n",
    "    path = path.replace(baseUrl, \"\")\n",
    "    path = downloadDirectory + path\n",
    "    directory = os.path.dirname(path)\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# html = urlopen(\"http://www.pythonscraping.com\")\n",
    "# bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "# downloadList = bsObj.findAll(src=True)\n",
    "\n",
    "# for download in downloadList:\n",
    "#     fileUrl = getAbsoluteURL(baseUrl, download[\"src\"])\n",
    "#     if fileUrl is not None:\n",
    "#         print(fileUrl)\n",
    "#         urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 실행하면 만나는 모든 파일을 내려받음..! 멀웨어 조심"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csvFile = open(\"test.csv\", \"w+\")\n",
    "try:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow((\"number\", \"number plus 2\", \"number times 2\"))\n",
    "    for i in range(10):\n",
    "        writer.writerow( (i, i+2, i*2 ))\n",
    "finally:\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML테이블 => CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html = urlopen(\"http://en.wikipedia.org/wiki/Comparison_of_text_editors\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "#The main comparison table is currently the first table on the page\n",
    "table = bsObj.findAll(\"table\",{\"class\":\"wikitable\"})[0]\n",
    "rows = table.findAll(\"tr\")\n",
    "\n",
    "csvFile = open(\"editors.csv\", 'wt', newline='', encoding='utf-8')\n",
    "writer = csv.writer(csvFile)\n",
    "try:\n",
    "    for row in rows:\n",
    "        csvRow = []\n",
    "        for cell in row.findAll(['td', 'th']):\n",
    "            csvRow.append(cell.get_text())\n",
    "        writer.writerow(csvRow)\n",
    "finally:\n",
    "    csvFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MySQL\n",
    "\n",
    "- 관계형 데이터베이스\n",
    "- http://dev.mysql.com/downloads/windows/installer/  설치 (setup type에서 Server Only 선택)\n",
    "\n",
    "\n",
    "#### 기본 명령어\n",
    "- CREATE DATABASE scraping\n",
    "- USE scraping\n",
    "- CREATE TABLE pages(id BIGINT(7) NOT NULL AUTO_INCREMENT, title VARCHAR(200), content VARCHAR(10000), created TIMESTAMP DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY(id));\n",
    "\n",
    "- 각 열의 정의는 이름 / 변수 타입 / 추가 속성 그리고 마지막에 키를 정의해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'test page title', 'this is some test page content. It can be up to 10,000 characters long.', datetime.datetime(2017, 1, 22, 16, 15, 52))\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "conn = pymysql.connect(host='127.0.0.1', user='root', passwd='900402', db='mysql', charset='utf8')\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"USE scraping\")\n",
    "cur.execute(\"SELECT * FROM pages WHERE id=1\")\n",
    "print(cur.fetchone())\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연결 객체 ( conn )\n",
    "- 데이터 베이스 연결에 관여 / 정보를 보내고 롤백을 처리하고 새 커서 객체를 만드는 역할\n",
    "\n",
    "\n",
    "커서 ( cur )\n",
    "- 상태 정보를 추적 / 마지막에 실행항 쿼리 결과\n",
    "- cur.fetchone() 으로 함수를 호출해 정보 접근 가능\n",
    "- 커서와 연결 사용을 마치면 이들을 닫아야 함..! => 연결 누수 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
